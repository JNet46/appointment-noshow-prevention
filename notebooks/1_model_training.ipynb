{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: All libraries imported successfully! ---\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: IMPORT ALL LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle  # <-- The critical import\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Step 1: All libraries imported successfully! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Dataset loaded successfully! ---\n",
      "Dataset has 10 rows and 9 columns.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: LOAD THE DATASET\n",
    "\n",
    "DATA_PATH = '../data/sample_appointments.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"--- Step 2: Dataset loaded successfully! ---\")\n",
    "print(f\"Dataset has {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Starting Data Preprocessing ---\n",
      "Found 'gender' column. Converting to numeric 'gender_encoded'.\n",
      "Dropping columns: ['appointment_id', 'gender']\n",
      "\n",
      "Data after preprocessing:\n",
      "   age  scholarship  hypertension  diabetes  sms_received  days_between  \\\n",
      "0   35            0             1         0             1            14   \n",
      "1   22            1             0         0             1             5   \n",
      "2   68            0             1         1             0             2   \n",
      "3   19            0             0         0             1            25   \n",
      "4   45            0             0         0             0             8   \n",
      "\n",
      "   no_show  gender_encoded  \n",
      "0        0               1  \n",
      "1        1               0  \n",
      "2        0               0  \n",
      "3        1               1  \n",
      "4        0               0  \n",
      "\n",
      "--- Step 3: Preprocessing Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA) & Preprocessing (CORRECTED)\n",
    "\n",
    "print(\"--- Step 3: Starting Data Preprocessing ---\")\n",
    "\n",
    "# Make a copy to avoid changing the original DataFrame\n",
    "df_processed = df.copy()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "# Convert 'gender' to a numeric format.\n",
    "# We will check if the column exists first to be safe.\n",
    "if 'gender' in df_processed.columns:\n",
    "    print(\"Found 'gender' column. Converting to numeric 'gender_encoded'.\")\n",
    "    df_processed['gender_encoded'] = df_processed['gender'].map({'M': 1, 'F': 0})\n",
    "else:\n",
    "    print(\"Warning: 'gender' column not found.\")\n",
    "\n",
    "# --- Column Dropping ---\n",
    "# Define columns to drop. We'll check if they exist before trying to drop them.\n",
    "cols_to_drop = ['appointment_id', 'gender']\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df_processed.columns]\n",
    "\n",
    "if existing_cols_to_drop:\n",
    "    print(f\"Dropping columns: {existing_cols_to_drop}\")\n",
    "    df_processed = df_processed.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "print(\"\\nData after preprocessing:\")\n",
    "print(df_processed.head())\n",
    "\n",
    "print(\"\\n--- Step 3: Preprocessing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Defining Features and Target ---\n",
      "SUCCESS: Target column 'no_show' found in the data.\n",
      "\n",
      "Features (X) created. Shape: (10, 7)\n",
      "First 5 rows of X:\n",
      "   age  scholarship  hypertension  diabetes  sms_received  days_between  \\\n",
      "0   35            0             1         0             1            14   \n",
      "1   22            1             0         0             1             5   \n",
      "2   68            0             1         1             0             2   \n",
      "3   19            0             0         0             1            25   \n",
      "4   45            0             0         0             0             8   \n",
      "\n",
      "   gender_encoded  \n",
      "0               1  \n",
      "1               0  \n",
      "2               0  \n",
      "3               1  \n",
      "4               0  \n",
      "\n",
      "Target (y) created. Shape: (10,)\n",
      "First 5 values of y:\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: no_show, dtype: int64\n",
      "\n",
      "--- Step 4: Features and Target Defined Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Features (X) and Target (y) (CORRECTED)\n",
    "\n",
    "print(\"--- Step 4: Defining Features and Target ---\")\n",
    "\n",
    "# Define the name of our target column\n",
    "TARGET = 'no_show'\n",
    "\n",
    "# --- Self-Check: Verify Target Column Exists ---\n",
    "if TARGET not in df_processed.columns:\n",
    "    print(f\"CRITICAL ERROR: The target column '{TARGET}' was NOT FOUND in the processed data.\")\n",
    "    print(f\"Available columns are: {df_processed.columns.tolist()}\")\n",
    "    # We will stop the process here by raising an error\n",
    "    raise KeyError(f\"Target column '{TARGET}' not found.\")\n",
    "else:\n",
    "    print(f\"SUCCESS: Target column '{TARGET}' found in the data.\")\n",
    "\n",
    "# --- Create X and y ---\n",
    "# y is the target column\n",
    "y = df_processed[TARGET]\n",
    "\n",
    "# X is all columns EXCEPT the target column\n",
    "X = df_processed.drop(columns=[TARGET])\n",
    "\n",
    "print(\"\\nFeatures (X) created. Shape:\", X.shape)\n",
    "print(\"First 5 rows of X:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nTarget (y) created. Shape:\", y.shape)\n",
    "print(\"First 5 values of y:\")\n",
    "print(y.head())\n",
    "\n",
    "print(\"\\n--- Step 4: Features and Target Defined Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 5: Splitting Data ---\n",
      "Data split successfully.\n",
      "X_train shape: (7, 7)\n",
      "X_test shape: (3, 7)\n",
      "y_train shape: (7,)\n",
      "y_test shape: (3,)\n",
      "--- Step 5: Splitting Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split Data into Training and Testing Sets (CORRECTED)\n",
    "\n",
    "print(\"--- Step 5: Splitting Data ---\")\n",
    "\n",
    "# Check if X and y exist before trying to split them\n",
    "if 'X' not in locals() or 'y' not in locals():\n",
    "    raise NameError(\"Critical Error: 'X' or 'y' not found. Cannot split data.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,      # Using 30% for testing with our small dataset\n",
    "    random_state=42, \n",
    "    stratify=y          # Important for keeping class balance\n",
    ")\n",
    "\n",
    "print(f\"Data split successfully.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"--- Step 5: Splitting Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 6: Training the Model ---\n",
      "Model initialized. Starting training...\n",
      "--- Step 6: Model Training Complete! ---\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Machine Learning Model (CORRECTED)\n",
    "\n",
    "print(\"--- Step 6: Training the Model ---\")\n",
    "\n",
    "# Check if training data exists\n",
    "if 'X_train' not in locals() or 'y_train' not in locals():\n",
    "    raise NameError(\"Critical Error: 'X_train' or 'y_train' not found. Cannot train model.\")\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Model initialized. Starting training...\")\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Step 6: Model Training Complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In a new Markdown cell\n",
    "# # Step 7: Evaluate the Model's Performance\n",
    "\n",
    "print(\"Evaluating model performance on the test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display a Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING ULTIMATE HEALTH CHECK ---\n",
      "\n",
      "[Check 1: Initial Data Load]\n",
      "  SUCCESS: 'df' DataFrame exists. Shape: (10, 9)\n",
      "\n",
      "[Check 2: Data Preprocessing]\n",
      "  SUCCESS: 'df_processed' DataFrame exists. Shape: (10, 8)\n",
      "\n",
      "[Check 3: Feature/Target Split]\n",
      "  SUCCESS: Features 'X' and target 'y' exist.\n",
      "  Shape of X: (10, 7)\n",
      "  Shape of y: (10,)\n",
      "\n",
      "[Check 4: Model Training Status]\n",
      "  SUCCESS: The 'model' object has been successfully fitted (trained).\n",
      "\n",
      "--- HEALTH CHECK COMPLETE ---\n",
      "✅ All checks passed! The model should be ready to save.\n"
     ]
    }
   ],
   "source": [
    "# ULTIMATE HEALTH CHECK CELL\n",
    "\n",
    "print(\"--- RUNNING ULTIMATE HEALTH CHECK ---\")\n",
    "all_checks_passed = True\n",
    "\n",
    "# Check 1: Was the initial DataFrame loaded correctly?\n",
    "print(\"\\n[Check 1: Initial Data Load]\")\n",
    "try:\n",
    "    if 'df' in locals() and not df.empty:\n",
    "        print(f\"  SUCCESS: 'df' DataFrame exists. Shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"  CRITICAL FAILURE: 'df' DataFrame is empty or does not exist.\")\n",
    "        all_checks_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"  CRITICAL FAILURE: Error checking 'df': {e}\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Check 2: Did preprocessing work?\n",
    "print(\"\\n[Check 2: Data Preprocessing]\")\n",
    "try:\n",
    "    if 'df_processed' in locals() and not df_processed.empty:\n",
    "        print(f\"  SUCCESS: 'df_processed' DataFrame exists. Shape: {df_processed.shape}\")\n",
    "        if df_processed.isnull().sum().sum() > 0:\n",
    "            print(\"  WARNING: There are missing values (NaNs) in the processed data!\")\n",
    "            all_checks_passed = False\n",
    "    else:\n",
    "        print(\"  CRITICAL FAILURE: 'df_processed' is empty or does not exist.\")\n",
    "        all_checks_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"  CRITICAL FAILURE: Error checking 'df_processed': {e}\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Check 3: Were features (X) and target (y) created correctly?\n",
    "print(\"\\n[Check 3: Feature/Target Split]\")\n",
    "try:\n",
    "    if 'X' in locals() and not X.empty and 'y' in locals() and not y.empty:\n",
    "        print(f\"  SUCCESS: Features 'X' and target 'y' exist.\")\n",
    "        print(f\"  Shape of X: {X.shape}\")\n",
    "        print(f\"  Shape of y: {y.shape}\")\n",
    "    else:\n",
    "        print(\"  CRITICAL FAILURE: 'X' or 'y' are empty or do not exist.\")\n",
    "        all_checks_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"  CRITICAL FAILURE: Error checking 'X' or 'y': {e}\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Check 4: Was the model trained successfully?\n",
    "print(\"\\n[Check 4: Model Training Status]\")\n",
    "try:\n",
    "    if 'model' in locals():\n",
    "        # The definitive test for a trained scikit-learn model\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            print(\"  SUCCESS: The 'model' object has been successfully fitted (trained).\")\n",
    "        else:\n",
    "            print(\"  CRITICAL FAILURE: 'model' exists, but it has NOT been fitted.\")\n",
    "            all_checks_passed = False\n",
    "    else:\n",
    "        print(\"  CRITICAL FAILURE: The 'model' variable does not exist.\")\n",
    "        all_checks_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"  CRITICAL FAILURE: Error checking 'model': {e}\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n--- HEALTH CHECK COMPLETE ---\")\n",
    "if all_checks_passed:\n",
    "    print(\"✅ All checks passed! The model should be ready to save.\")\n",
    "else:\n",
    "    print(\"❌ One or more critical checks failed. The model will NOT be saved correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved successfully to: ../trained_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# In a new Markdown cell\n",
    "# # Step 8: Save the Trained Model for the Streamlit App\n",
    "\n",
    "# This is the final and most important step. We serialize the trained model object\n",
    "# using pickle and save it to a file. Our Streamlit app will load this file to make predictions.\n",
    "\n",
    "MODEL_PATH = '../trained_model.pkl'\n",
    "\n",
    "with open(MODEL_PATH, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"\\nModel saved successfully to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
